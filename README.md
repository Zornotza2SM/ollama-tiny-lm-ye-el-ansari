Qué modelo has usado?

He usado:
TinyLlama,
phi.

Cuánto ocupa en disco ese modelo?

Según la biblioteca de modelos de Ollama, estos tamaños aproximados son:

Phi: ~2 GB

TinyLlama: ~1 GB

Crees que esta IA responde más rápido o más lento que ChatGPT? ¿Por qué?

Creo que esta IA local responde más lento que ChatGPT.
Esto se debe a que TinyLlama y Phi se ejecutan en mi propio ordenador, normalmente usando CPU y con menos recursos que los servidores de ChatGPT.
ChatGPT utiliza modelos mucho más grandes y servidores muy potentes con GPUs, lo que hace que sus respuestas sean más rápidas y precisas.
